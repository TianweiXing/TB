{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf cifar10 structure, fedlearn cifar10 similar structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm as cmap\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10, mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fully_connected(input_node, num_outputs, scope=None):\n",
    "    \"\"\" Implements a fully connected layer. \"\"\"\n",
    "    w_initializer = tf.truncated_normal_initializer(stddev=0.04)\n",
    "    b_initializer = tf.constant_initializer(0.0)\n",
    "    input_shape = input_node.get_shape()\n",
    "    with tf.variable_scope(scope or 'fc'):\n",
    "        w = tf.get_variable(name='w', shape=[input_shape[1], num_outputs], initializer=w_initializer)\n",
    "        b = tf.get_variable(name='b', shape=[num_outputs], initializer=b_initializer)\n",
    "    fc = tf.matmul(input_node, w) + b\n",
    "    return fc, (w, b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv_layer(input_node, num_filters, filter_sz, stride=1, scope=None):\n",
    "    \"\"\" Implements a convolution layer. \"\"\"\n",
    "    w_initializer = tf.truncated_normal_initializer(stddev=5e-2)\n",
    "    b_initializer = tf.constant_initializer(0.0)\n",
    "    input_shape = input_node.get_shape()  # input_shape supposed to be [batch, h, w, d]\n",
    "    with tf.variable_scope(scope or 'conv'):\n",
    "        w = tf.get_variable(name='w', shape=[filter_sz, filter_sz, input_shape[-1], num_filters], initializer=w_initializer)\n",
    "        b = tf.get_variable(name='b', shape=[num_filters] , initializer=b_initializer)\n",
    "        conv_out = tf.nn.conv2d(input_node, w, strides=[1, stride, stride, 1], padding='SAME')\n",
    "        #conv_out = conv_out + b\n",
    "        conv_out = tf.nn.bias_add(conv_out, b)\n",
    "    return conv_out, (w, b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "390\n"
     ]
    }
   ],
   "source": [
    "# (train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
    "\n",
    "train_x = train_x.reshape((-1, 32, 32, 3)).astype(np.float32) / 255.0\n",
    "test_x = test_x.reshape((-1, 32, 32, 3)).astype(np.float32) / 255.0\n",
    "batch_size = 128\n",
    "\n",
    "global_step = 400\n",
    "init_learning_rate = 0.1\n",
    "lr_decay = 0.1\n",
    "\n",
    "reg_beta = 0.004\n",
    "\n",
    "train_y = np_utils.to_categorical(train_y, 10)\n",
    "test_y = np_utils.to_categorical(test_y, 10)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_x.shape[0]//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Variables is  10\n",
      "\n",
      "All variables\n",
      "<tf.Variable 'conv1/w:0' shape=(5, 5, 3, 64) dtype=float32_ref>\n",
      "<tf.Variable 'conv1/b:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'conv2/w:0' shape=(5, 5, 64, 64) dtype=float32_ref>\n",
      "<tf.Variable 'conv2/b:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'fc_3/w:0' shape=(4096, 384) dtype=float32_ref>\n",
      "<tf.Variable 'fc_3/b:0' shape=(384,) dtype=float32_ref>\n",
      "<tf.Variable 'fc_4/w:0' shape=(384, 192) dtype=float32_ref>\n",
      "<tf.Variable 'fc_4/b:0' shape=(192,) dtype=float32_ref>\n",
      "<tf.Variable 'soft_max/w:0' shape=(192, 10) dtype=float32_ref>\n",
      "<tf.Variable 'soft_max/b:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# reset default-graph\n",
    "tf.reset_default_graph()\n",
    "# with tf.Graph().as_default():\n",
    "\n",
    "# setting random seed:\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# place-holder for mnist data\n",
    "x_holder = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y_holder = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# network: (conv-pool-norm)-(conv-norm-pool)-fc-fc-fc\n",
    "with tf.name_scope('conv1'):\n",
    "    conv1, (w1, b1) = conv_layer(x_holder, 64, 5, 1, 'conv1')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    mp1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1,2, 2,1], padding='SAME')\n",
    "    norm1 = tf.nn.lrn(mp1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "\n",
    "with tf.name_scope('conv2'):\n",
    "    conv2, (w2, b2) = conv_layer(mp1, 64, 5, 1, 'conv2')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "    mp2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2,1], padding='SAME')\n",
    "\n",
    "mp_shape = mp2.get_shape()\n",
    "flat_dim = int(mp_shape[1] * mp_shape[2] * mp_shape[3])\n",
    "flat_conv = tf.reshape(mp2, [-1, flat_dim])\n",
    "\n",
    "with tf.name_scope('fc_3'):\n",
    "    fc3, (w3, b3) = fully_connected(flat_conv, 384, scope='fc_3')\n",
    "fc3 = tf.nn.relu(fc3)   \n",
    "\n",
    "with tf.name_scope('fc_4'):\n",
    "    fc4, (w4, b4) = fully_connected(fc3, 192, scope='fc_4')\n",
    "fc4 = tf.nn.relu(fc4)   \n",
    "    \n",
    "with tf.name_scope('soft_max'):\n",
    "    fc5, (w5, b5) = fully_connected(fc4, 10, scope='soft_max')\n",
    "\n",
    "logits = fc5\n",
    "preds = tf.nn.softmax(logits)\n",
    "correct_preds = tf.equal(tf.argmax(logits, 1), tf.argmax(y_holder, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_holder,\n",
    "                                            logits=logits)\n",
    ")\n",
    "\n",
    "# adding regularizer to cost function\n",
    "regularizer = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) + tf.nn.l2_loss(w3) + tf.nn.l2_loss(w4) + tf.nn.l2_loss(w5)\n",
    "loss = tf.reduce_mean(loss + reg_beta * regularizer)\n",
    "\n",
    "\n",
    "# # exponential decay for learning rate\n",
    "# # init_learning_rate = 0.1\n",
    "# # global_step = 10000\n",
    "# # global_step = 10000\n",
    "# learning_rate_use = tf.train.exponential_decay(init_learning_rate, \n",
    "#                                                global_step,\n",
    "#                                                1, 0.3, \n",
    "#                                                staircase=True)\n",
    "\n",
    "# change learning rate during training:\n",
    "lr_holder = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "\n",
    "\n",
    "# regular training (from scratch) using vanila SGD\n",
    "\n",
    "# using momentum\n",
    "# opt = tf.train.MomentumOptimizer(learning_rate,0.9)                                 \n",
    "opt = tf.train.GradientDescentOptimizer(lr_holder)\n",
    "train_op = opt.minimize(loss)\n",
    "\n",
    "all_variable = tf.trainable_variables()\n",
    "all_var_num = len(all_variable)\n",
    "print('Number of Variables is ', len(all_variable))\n",
    "print('\\nAll variables')\n",
    "for var in (all_variable):\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No:  0  Train_loss:8.729e+00  Cost:6.717e+00  Acc:0.34    LR:0.100\n",
      "No:  1  Train_loss:5.292e+00  Cost:4.170e+00  Acc:0.49    LR:0.100\n",
      "No:  2  Train_loss:3.496e+00  Cost:2.913e+00  Acc:0.52    LR:0.100\n",
      "No:  3  Train_loss:2.539e+00  Cost:2.258e+00  Acc:0.56    LR:0.100\n",
      "No:  4  Train_loss:2.006e+00  Cost:1.785e+00  Acc:0.62    LR:0.100\n",
      "No:  5  Train_loss:1.733e+00  Cost:1.656e+00  Acc:0.60    LR:0.100\n",
      "No:  6  Train_loss:1.553e+00  Cost:1.508e+00  Acc:0.64    LR:0.100\n",
      "No:  7  Train_loss:1.456e+00  Cost:1.475e+00  Acc:0.64    LR:0.100\n",
      "No:  8  Train_loss:1.395e+00  Cost:1.423e+00  Acc:0.66    LR:0.100\n",
      "No:  9  Train_loss:1.380e+00  Cost:1.906e+00  Acc:0.48    LR:0.100\n",
      "No: 10  Train_loss:1.336e+00  Cost:1.424e+00  Acc:0.66    LR:0.100\n",
      "No: 11  Train_loss:1.325e+00  Cost:1.511e+00  Acc:0.63    LR:0.100\n",
      "No: 12  Train_loss:1.309e+00  Cost:1.657e+00  Acc:0.60    LR:0.100\n",
      "No: 13  Train_loss:1.306e+00  Cost:1.407e+00  Acc:0.68    LR:0.100\n",
      "No: 14  Train_loss:1.303e+00  Cost:1.341e+00  Acc:0.71    LR:0.100\n",
      "No: 15  Train_loss:1.287e+00  Cost:1.364e+00  Acc:0.70    LR:0.100\n",
      "No: 16  Train_loss:1.283e+00  Cost:1.373e+00  Acc:0.70    LR:0.100\n",
      "No: 17  Train_loss:1.275e+00  Cost:1.355e+00  Acc:0.71    LR:0.100\n",
      "No: 18  Train_loss:1.271e+00  Cost:1.375e+00  Acc:0.70    LR:0.100\n",
      "No: 19  Train_loss:1.280e+00  Cost:1.529e+00  Acc:0.65    LR:0.100\n",
      "No: 20  Train_loss:1.271e+00  Cost:1.385e+00  Acc:0.71    LR:0.100\n",
      "No: 21  Train_loss:1.278e+00  Cost:1.362e+00  Acc:0.72    LR:0.100\n",
      "No: 22  Train_loss:1.270e+00  Cost:1.473e+00  Acc:0.68    LR:0.100\n",
      "No: 23  Train_loss:1.273e+00  Cost:1.471e+00  Acc:0.68    LR:0.100\n",
      "No: 24  Train_loss:1.258e+00  Cost:1.392e+00  Acc:0.71    LR:0.100\n",
      "No: 25  Train_loss:1.277e+00  Cost:1.373e+00  Acc:0.72    LR:0.100\n",
      "No: 26  Train_loss:1.289e+00  Cost:1.450e+00  Acc:0.69    LR:0.100\n",
      "No: 27  Train_loss:1.262e+00  Cost:1.516e+00  Acc:0.67    LR:0.100\n",
      "No: 28  Train_loss:1.265e+00  Cost:1.466e+00  Acc:0.68    LR:0.100\n",
      "No: 29  Train_loss:1.250e+00  Cost:1.425e+00  Acc:0.71    LR:0.100\n",
      "No: 30  Train_loss:1.264e+00  Cost:1.433e+00  Acc:0.70    LR:0.100\n",
      "No: 31  Train_loss:1.246e+00  Cost:1.441e+00  Acc:0.70    LR:0.100\n",
      "No: 32  Train_loss:1.245e+00  Cost:1.352e+00  Acc:0.73    LR:0.100\n",
      "No: 33  Train_loss:1.269e+00  Cost:1.440e+00  Acc:0.71    LR:0.100\n",
      "No: 34  Train_loss:1.327e+00  Cost:1.546e+00  Acc:0.66    LR:0.100\n",
      "No: 35  Train_loss:1.258e+00  Cost:1.396e+00  Acc:0.72    LR:0.100\n",
      "No: 36  Train_loss:1.261e+00  Cost:1.389e+00  Acc:0.73    LR:0.100\n",
      "No: 37  Train_loss:1.245e+00  Cost:1.438e+00  Acc:0.70    LR:0.100\n",
      "No: 38  Train_loss:1.252e+00  Cost:1.403e+00  Acc:0.72    LR:0.100\n",
      "No: 39  Train_loss:1.247e+00  Cost:1.450e+00  Acc:0.70    LR:0.100\n",
      "No: 40  Train_loss:1.287e+00  Cost:1.351e+00  Acc:0.74    LR:0.100\n",
      "No: 41  Train_loss:1.238e+00  Cost:1.376e+00  Acc:0.73    LR:0.100\n",
      "No: 42  Train_loss:1.239e+00  Cost:1.386e+00  Acc:0.73    LR:0.100\n",
      "No: 43  Train_loss:1.237e+00  Cost:1.455e+00  Acc:0.71    LR:0.100\n",
      "No: 44  Train_loss:1.240e+00  Cost:1.449e+00  Acc:0.70    LR:0.100\n",
      "No: 45  Train_loss:1.264e+00  Cost:1.441e+00  Acc:0.70    LR:0.100\n",
      "No: 46  Train_loss:1.242e+00  Cost:1.461e+00  Acc:0.70    LR:0.100\n",
      "No: 47  Train_loss:1.252e+00  Cost:1.544e+00  Acc:0.68    LR:0.100\n",
      "No: 48  Train_loss:1.252e+00  Cost:1.436e+00  Acc:0.72    LR:0.100\n",
      "No: 49  Train_loss:1.245e+00  Cost:1.442e+00  Acc:0.71    LR:0.100\n",
      "No: 50  Train_loss:1.236e+00  Cost:1.763e+00  Acc:0.62    LR:0.100\n",
      "No: 51  Train_loss:1.234e+00  Cost:1.463e+00  Acc:0.70    LR:0.100\n",
      "No: 52  Train_loss:1.246e+00  Cost:1.552e+00  Acc:0.67    LR:0.100\n",
      "No: 53  Train_loss:1.233e+00  Cost:1.438e+00  Acc:0.72    LR:0.100\n",
      "No: 54  Train_loss:1.233e+00  Cost:1.383e+00  Acc:0.73    LR:0.100\n",
      "No: 55  Train_loss:1.232e+00  Cost:1.389e+00  Acc:0.73    LR:0.100\n",
      "No: 56  Train_loss:1.231e+00  Cost:1.494e+00  Acc:0.70    LR:0.100\n",
      "No: 57  Train_loss:1.230e+00  Cost:1.456e+00  Acc:0.71    LR:0.100\n",
      "No: 58  Train_loss:1.228e+00  Cost:1.422e+00  Acc:0.72    LR:0.100\n",
      "No: 59  Train_loss:1.236e+00  Cost:1.476e+00  Acc:0.70    LR:0.100\n",
      "No: 60  Train_loss:1.241e+00  Cost:1.421e+00  Acc:0.72    LR:0.100\n",
      "No: 61  Train_loss:1.235e+00  Cost:1.554e+00  Acc:0.67    LR:0.100\n",
      "No: 62  Train_loss:1.228e+00  Cost:1.425e+00  Acc:0.72    LR:0.100\n",
      "No: 63  Train_loss:1.249e+00  Cost:1.450e+00  Acc:0.72    LR:0.100\n",
      "No: 64  Train_loss:1.233e+00  Cost:1.422e+00  Acc:0.73    LR:0.100\n",
      "No: 65  Train_loss:1.232e+00  Cost:1.495e+00  Acc:0.70    LR:0.100\n",
      "No: 66  Train_loss:1.224e+00  Cost:1.519e+00  Acc:0.69    LR:0.100\n",
      "No: 67  Train_loss:1.229e+00  Cost:1.378e+00  Acc:0.74    LR:0.100\n",
      "No: 68  Train_loss:1.251e+00  Cost:1.471e+00  Acc:0.71    LR:0.100\n",
      "No: 69  Train_loss:1.224e+00  Cost:1.560e+00  Acc:0.67    LR:0.100\n",
      "No: 70  Train_loss:1.247e+00  Cost:1.436e+00  Acc:0.72    LR:0.100\n",
      "No: 71  Train_loss:1.228e+00  Cost:1.409e+00  Acc:0.73    LR:0.100\n",
      "No: 72  Train_loss:1.220e+00  Cost:1.382e+00  Acc:0.74    LR:0.100\n",
      "No: 73  Train_loss:1.234e+00  Cost:1.517e+00  Acc:0.69    LR:0.100\n",
      "No: 74  Train_loss:1.232e+00  Cost:1.405e+00  Acc:0.73    LR:0.100\n",
      "No: 75  Train_loss:1.223e+00  Cost:1.532e+00  Acc:0.69    LR:0.100\n",
      "No: 76  Train_loss:1.248e+00  Cost:1.419e+00  Acc:0.72    LR:0.100\n",
      "No: 77  Train_loss:1.210e+00  Cost:1.384e+00  Acc:0.74    LR:0.100\n",
      "No: 78  Train_loss:1.218e+00  Cost:1.470e+00  Acc:0.71    LR:0.100\n",
      "No: 79  Train_loss:1.233e+00  Cost:1.544e+00  Acc:0.69    LR:0.100\n",
      "No: 80  Train_loss:1.234e+00  Cost:1.428e+00  Acc:0.73    LR:0.100\n",
      "No: 81  Train_loss:1.234e+00  Cost:1.656e+00  Acc:0.66    LR:0.100\n",
      "No: 82  Train_loss:1.214e+00  Cost:1.640e+00  Acc:0.66    LR:0.100\n",
      "No: 83  Train_loss:1.224e+00  Cost:1.460e+00  Acc:0.71    LR:0.100\n",
      "No: 84  Train_loss:1.220e+00  Cost:1.454e+00  Acc:0.71    LR:0.100\n",
      "No: 85  Train_loss:1.237e+00  Cost:1.465e+00  Acc:0.71    LR:0.100\n",
      "No: 86  Train_loss:1.234e+00  Cost:1.622e+00  Acc:0.67    LR:0.100\n",
      "No: 87  Train_loss:1.225e+00  Cost:1.508e+00  Acc:0.69    LR:0.100\n",
      "No: 88  Train_loss:1.205e+00  Cost:1.409e+00  Acc:0.73    LR:0.100\n",
      "No: 89  Train_loss:1.253e+00  Cost:1.599e+00  Acc:0.68    LR:0.100\n",
      "No: 90  Train_loss:1.220e+00  Cost:1.575e+00  Acc:0.67    LR:0.100\n",
      "No: 91  Train_loss:1.230e+00  Cost:1.432e+00  Acc:0.72    LR:0.100\n",
      "No: 92  Train_loss:1.219e+00  Cost:1.402e+00  Acc:0.73    LR:0.100\n",
      "No: 93  Train_loss:1.222e+00  Cost:1.452e+00  Acc:0.72    LR:0.100\n",
      "No: 94  Train_loss:1.228e+00  Cost:1.476e+00  Acc:0.71    LR:0.100\n",
      "No: 95  Train_loss:1.231e+00  Cost:1.417e+00  Acc:0.73    LR:0.100\n",
      "No: 96  Train_loss:1.240e+00  Cost:1.451e+00  Acc:0.72    LR:0.100\n",
      "No: 97  Train_loss:1.222e+00  Cost:1.595e+00  Acc:0.68    LR:0.100\n",
      "No: 98  Train_loss:1.227e+00  Cost:1.558e+00  Acc:0.69    LR:0.100\n",
      "No: 99  Train_loss:1.230e+00  Cost:1.407e+00  Acc:0.74    LR:0.100\n",
      "No:100  Train_loss:1.031e+00  Cost:1.299e+00  Acc:0.77    LR:0.010\n",
      "No:101  Train_loss:9.750e-01  Cost:1.282e+00  Acc:0.78    LR:0.010\n",
      "No:102  Train_loss:9.605e-01  Cost:1.266e+00  Acc:0.78    LR:0.010\n",
      "No:103  Train_loss:9.398e-01  Cost:1.257e+00  Acc:0.78    LR:0.010\n",
      "No:104  Train_loss:9.209e-01  Cost:1.247e+00  Acc:0.78    LR:0.010\n",
      "No:105  Train_loss:9.067e-01  Cost:1.253e+00  Acc:0.78    LR:0.010\n",
      "No:106  Train_loss:8.991e-01  Cost:1.244e+00  Acc:0.78    LR:0.010\n",
      "No:107  Train_loss:8.893e-01  Cost:1.238e+00  Acc:0.78    LR:0.010\n",
      "No:108  Train_loss:8.804e-01  Cost:1.239e+00  Acc:0.78    LR:0.010\n",
      "No:109  Train_loss:8.751e-01  Cost:1.246e+00  Acc:0.78    LR:0.010\n",
      "No:110  Train_loss:8.635e-01  Cost:1.240e+00  Acc:0.78    LR:0.010\n",
      "No:111  Train_loss:8.567e-01  Cost:1.243e+00  Acc:0.78    LR:0.010\n",
      "No:112  Train_loss:8.528e-01  Cost:1.246e+00  Acc:0.78    LR:0.010\n",
      "No:113  Train_loss:8.466e-01  Cost:1.240e+00  Acc:0.78    LR:0.010\n",
      "No:114  Train_loss:8.408e-01  Cost:1.245e+00  Acc:0.78    LR:0.010\n",
      "No:115  Train_loss:8.397e-01  Cost:1.242e+00  Acc:0.78    LR:0.010\n",
      "No:116  Train_loss:8.369e-01  Cost:1.254e+00  Acc:0.78    LR:0.010\n",
      "No:117  Train_loss:8.278e-01  Cost:1.259e+00  Acc:0.78    LR:0.010\n",
      "No:118  Train_loss:8.312e-01  Cost:1.255e+00  Acc:0.78    LR:0.010\n",
      "No:119  Train_loss:8.239e-01  Cost:1.249e+00  Acc:0.78    LR:0.010\n",
      "No:120  Train_loss:8.225e-01  Cost:1.276e+00  Acc:0.77    LR:0.010\n",
      "No:121  Train_loss:8.210e-01  Cost:1.264e+00  Acc:0.78    LR:0.010\n",
      "No:122  Train_loss:8.176e-01  Cost:1.262e+00  Acc:0.78    LR:0.010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No:123  Train_loss:8.150e-01  Cost:1.264e+00  Acc:0.78    LR:0.010\n",
      "No:124  Train_loss:8.132e-01  Cost:1.254e+00  Acc:0.78    LR:0.010\n",
      "No:125  Train_loss:8.129e-01  Cost:1.276e+00  Acc:0.77    LR:0.010\n",
      "No:126  Train_loss:8.112e-01  Cost:1.253e+00  Acc:0.78    LR:0.010\n",
      "No:127  Train_loss:8.058e-01  Cost:1.273e+00  Acc:0.78    LR:0.010\n",
      "No:128  Train_loss:8.052e-01  Cost:1.266e+00  Acc:0.78    LR:0.010\n",
      "No:129  Train_loss:8.057e-01  Cost:1.261e+00  Acc:0.78    LR:0.010\n",
      "No:130  Train_loss:8.034e-01  Cost:1.270e+00  Acc:0.78    LR:0.010\n",
      "No:131  Train_loss:8.005e-01  Cost:1.274e+00  Acc:0.77    LR:0.010\n",
      "No:132  Train_loss:7.993e-01  Cost:1.274e+00  Acc:0.78    LR:0.010\n",
      "No:133  Train_loss:8.018e-01  Cost:1.273e+00  Acc:0.78    LR:0.010\n",
      "No:134  Train_loss:8.000e-01  Cost:1.304e+00  Acc:0.76    LR:0.010\n",
      "No:135  Train_loss:7.966e-01  Cost:1.273e+00  Acc:0.78    LR:0.010\n",
      "No:136  Train_loss:7.976e-01  Cost:1.289e+00  Acc:0.78    LR:0.010\n",
      "No:137  Train_loss:7.942e-01  Cost:1.274e+00  Acc:0.78    LR:0.010\n",
      "No:138  Train_loss:7.920e-01  Cost:1.281e+00  Acc:0.77    LR:0.010\n",
      "No:139  Train_loss:7.925e-01  Cost:1.281e+00  Acc:0.78    LR:0.010\n",
      "No:140  Train_loss:7.923e-01  Cost:1.294e+00  Acc:0.77    LR:0.010\n",
      "No:141  Train_loss:7.909e-01  Cost:1.305e+00  Acc:0.77    LR:0.010\n",
      "No:142  Train_loss:7.878e-01  Cost:1.278e+00  Acc:0.78    LR:0.010\n",
      "No:143  Train_loss:7.911e-01  Cost:1.317e+00  Acc:0.77    LR:0.010\n",
      "No:144  Train_loss:7.905e-01  Cost:1.287e+00  Acc:0.78    LR:0.010\n",
      "No:145  Train_loss:7.920e-01  Cost:1.277e+00  Acc:0.77    LR:0.010\n",
      "No:146  Train_loss:7.869e-01  Cost:1.295e+00  Acc:0.77    LR:0.010\n",
      "No:147  Train_loss:7.926e-01  Cost:1.278e+00  Acc:0.78    LR:0.010\n",
      "No:148  Train_loss:7.837e-01  Cost:1.318e+00  Acc:0.76    LR:0.010\n",
      "No:149  Train_loss:7.864e-01  Cost:1.288e+00  Acc:0.78    LR:0.010\n",
      "No:150  Train_loss:7.879e-01  Cost:1.285e+00  Acc:0.78    LR:0.010\n",
      "No:151  Train_loss:7.855e-01  Cost:1.291e+00  Acc:0.77    LR:0.010\n",
      "No:152  Train_loss:7.850e-01  Cost:1.302e+00  Acc:0.77    LR:0.010\n",
      "No:153  Train_loss:7.850e-01  Cost:1.289e+00  Acc:0.77    LR:0.010\n",
      "No:154  Train_loss:7.820e-01  Cost:1.285e+00  Acc:0.77    LR:0.010\n",
      "No:155  Train_loss:7.866e-01  Cost:1.288e+00  Acc:0.77    LR:0.010\n",
      "No:156  Train_loss:7.833e-01  Cost:1.327e+00  Acc:0.77    LR:0.010\n",
      "No:157  Train_loss:7.849e-01  Cost:1.302e+00  Acc:0.77    LR:0.010\n",
      "No:158  Train_loss:7.784e-01  Cost:1.290e+00  Acc:0.77    LR:0.010\n",
      "No:159  Train_loss:7.839e-01  Cost:1.306e+00  Acc:0.77    LR:0.010\n",
      "No:160  Train_loss:7.822e-01  Cost:1.286e+00  Acc:0.78    LR:0.010\n",
      "No:161  Train_loss:7.791e-01  Cost:1.316e+00  Acc:0.77    LR:0.010\n",
      "No:162  Train_loss:7.795e-01  Cost:1.317e+00  Acc:0.77    LR:0.010\n",
      "No:163  Train_loss:7.768e-01  Cost:1.292e+00  Acc:0.77    LR:0.010\n",
      "No:164  Train_loss:7.818e-01  Cost:1.286e+00  Acc:0.77    LR:0.010\n",
      "No:165  Train_loss:7.805e-01  Cost:1.316e+00  Acc:0.77    LR:0.010\n",
      "No:166  Train_loss:7.835e-01  Cost:1.326e+00  Acc:0.77    LR:0.010\n",
      "No:167  Train_loss:7.793e-01  Cost:1.295e+00  Acc:0.77    LR:0.010\n",
      "No:168  Train_loss:7.794e-01  Cost:1.369e+00  Acc:0.76    LR:0.010\n",
      "No:169  Train_loss:7.773e-01  Cost:1.326e+00  Acc:0.77    LR:0.010\n",
      "No:170  Train_loss:7.794e-01  Cost:1.361e+00  Acc:0.76    LR:0.010\n",
      "No:171  Train_loss:7.764e-01  Cost:1.301e+00  Acc:0.77    LR:0.010\n",
      "No:172  Train_loss:7.773e-01  Cost:1.310e+00  Acc:0.77    LR:0.010\n",
      "No:173  Train_loss:7.768e-01  Cost:1.310e+00  Acc:0.77    LR:0.010\n",
      "No:174  Train_loss:7.843e-01  Cost:1.298e+00  Acc:0.77    LR:0.010\n",
      "No:175  Train_loss:7.774e-01  Cost:1.317e+00  Acc:0.77    LR:0.010\n",
      "No:176  Train_loss:7.767e-01  Cost:1.302e+00  Acc:0.77    LR:0.010\n",
      "No:177  Train_loss:7.759e-01  Cost:1.342e+00  Acc:0.76    LR:0.010\n",
      "No:178  Train_loss:7.757e-01  Cost:1.345e+00  Acc:0.76    LR:0.010\n",
      "No:179  Train_loss:7.757e-01  Cost:1.316e+00  Acc:0.77    LR:0.010\n",
      "No:180  Train_loss:7.726e-01  Cost:1.324e+00  Acc:0.77    LR:0.010\n",
      "No:181  Train_loss:7.779e-01  Cost:1.308e+00  Acc:0.77    LR:0.010\n",
      "No:182  Train_loss:7.723e-01  Cost:1.318e+00  Acc:0.77    LR:0.010\n",
      "No:183  Train_loss:7.737e-01  Cost:1.318e+00  Acc:0.77    LR:0.010\n",
      "No:184  Train_loss:7.755e-01  Cost:1.367e+00  Acc:0.76    LR:0.010\n",
      "No:185  Train_loss:7.720e-01  Cost:1.310e+00  Acc:0.77    LR:0.010\n",
      "No:186  Train_loss:7.740e-01  Cost:1.319e+00  Acc:0.77    LR:0.010\n",
      "No:187  Train_loss:7.772e-01  Cost:1.313e+00  Acc:0.77    LR:0.010\n",
      "No:188  Train_loss:7.700e-01  Cost:1.328e+00  Acc:0.77    LR:0.010\n",
      "No:189  Train_loss:7.706e-01  Cost:1.329e+00  Acc:0.77    LR:0.010\n",
      "No:190  Train_loss:7.670e-01  Cost:1.396e+00  Acc:0.75    LR:0.010\n",
      "No:191  Train_loss:7.820e-01  Cost:1.320e+00  Acc:0.77    LR:0.010\n",
      "No:192  Train_loss:7.801e-01  Cost:1.314e+00  Acc:0.77    LR:0.010\n",
      "No:193  Train_loss:7.758e-01  Cost:1.377e+00  Acc:0.75    LR:0.010\n",
      "No:194  Train_loss:7.805e-01  Cost:1.357e+00  Acc:0.76    LR:0.010\n",
      "No:195  Train_loss:7.727e-01  Cost:1.315e+00  Acc:0.77    LR:0.010\n",
      "No:196  Train_loss:7.714e-01  Cost:1.331e+00  Acc:0.77    LR:0.010\n",
      "No:197  Train_loss:7.799e-01  Cost:1.343e+00  Acc:0.76    LR:0.010\n",
      "No:198  Train_loss:7.723e-01  Cost:1.334e+00  Acc:0.76    LR:0.010\n",
      "No:199  Train_loss:7.674e-01  Cost:1.312e+00  Acc:0.77    LR:0.010\n",
      "No:200  Train_loss:7.356e-01  Cost:1.291e+00  Acc:0.78    LR:0.001\n",
      "No:201  Train_loss:7.313e-01  Cost:1.291e+00  Acc:0.78    LR:0.001\n",
      "No:202  Train_loss:7.284e-01  Cost:1.292e+00  Acc:0.78    LR:0.001\n",
      "No:203  Train_loss:7.276e-01  Cost:1.292e+00  Acc:0.78    LR:0.001\n",
      "No:204  Train_loss:7.262e-01  Cost:1.292e+00  Acc:0.78    LR:0.001\n",
      "No:205  Train_loss:7.256e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:206  Train_loss:7.234e-01  Cost:1.293e+00  Acc:0.78    LR:0.001\n",
      "No:207  Train_loss:7.242e-01  Cost:1.295e+00  Acc:0.78    LR:0.001\n",
      "No:208  Train_loss:7.238e-01  Cost:1.293e+00  Acc:0.78    LR:0.001\n",
      "No:209  Train_loss:7.230e-01  Cost:1.293e+00  Acc:0.78    LR:0.001\n",
      "No:210  Train_loss:7.226e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:211  Train_loss:7.207e-01  Cost:1.293e+00  Acc:0.78    LR:0.001\n",
      "No:212  Train_loss:7.200e-01  Cost:1.292e+00  Acc:0.78    LR:0.001\n",
      "No:213  Train_loss:7.219e-01  Cost:1.293e+00  Acc:0.78    LR:0.001\n",
      "No:214  Train_loss:7.204e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:215  Train_loss:7.190e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:216  Train_loss:7.209e-01  Cost:1.291e+00  Acc:0.78    LR:0.001\n",
      "No:217  Train_loss:7.188e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:218  Train_loss:7.187e-01  Cost:1.293e+00  Acc:0.78    LR:0.001\n",
      "No:219  Train_loss:7.189e-01  Cost:1.291e+00  Acc:0.78    LR:0.001\n",
      "No:220  Train_loss:7.177e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:221  Train_loss:7.186e-01  Cost:1.291e+00  Acc:0.78    LR:0.001\n",
      "No:222  Train_loss:7.187e-01  Cost:1.291e+00  Acc:0.78    LR:0.001\n",
      "No:223  Train_loss:7.195e-01  Cost:1.291e+00  Acc:0.78    LR:0.001\n",
      "No:224  Train_loss:7.171e-01  Cost:1.291e+00  Acc:0.78    LR:0.001\n",
      "No:225  Train_loss:7.160e-01  Cost:1.291e+00  Acc:0.78    LR:0.001\n",
      "No:226  Train_loss:7.154e-01  Cost:1.291e+00  Acc:0.78    LR:0.001\n",
      "No:227  Train_loss:7.161e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:228  Train_loss:7.158e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:229  Train_loss:7.157e-01  Cost:1.293e+00  Acc:0.78    LR:0.001\n",
      "No:230  Train_loss:7.161e-01  Cost:1.290e+00  Acc:0.78    LR:0.001\n",
      "No:231  Train_loss:7.137e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:232  Train_loss:7.153e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:233  Train_loss:7.150e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:234  Train_loss:7.133e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:235  Train_loss:7.134e-01  Cost:1.293e+00  Acc:0.78    LR:0.001\n",
      "No:236  Train_loss:7.152e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:237  Train_loss:7.126e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:238  Train_loss:7.136e-01  Cost:1.290e+00  Acc:0.78    LR:0.001\n",
      "No:239  Train_loss:7.134e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:240  Train_loss:7.126e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:241  Train_loss:7.125e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:242  Train_loss:7.129e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:243  Train_loss:7.127e-01  Cost:1.289e+00  Acc:0.78    LR:0.001\n",
      "No:244  Train_loss:7.121e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:245  Train_loss:7.120e-01  Cost:1.290e+00  Acc:0.78    LR:0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No:246  Train_loss:7.112e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:247  Train_loss:7.137e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:248  Train_loss:7.120e-01  Cost:1.289e+00  Acc:0.78    LR:0.001\n",
      "No:249  Train_loss:7.106e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:250  Train_loss:7.117e-01  Cost:1.289e+00  Acc:0.78    LR:0.001\n",
      "No:251  Train_loss:7.103e-01  Cost:1.289e+00  Acc:0.78    LR:0.001\n",
      "No:252  Train_loss:7.105e-01  Cost:1.288e+00  Acc:0.78    LR:0.001\n",
      "No:253  Train_loss:7.117e-01  Cost:1.289e+00  Acc:0.78    LR:0.001\n",
      "No:254  Train_loss:7.102e-01  Cost:1.287e+00  Acc:0.77    LR:0.001\n",
      "No:255  Train_loss:7.112e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:256  Train_loss:7.096e-01  Cost:1.289e+00  Acc:0.78    LR:0.001\n",
      "No:257  Train_loss:7.108e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:258  Train_loss:7.101e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:259  Train_loss:7.096e-01  Cost:1.288e+00  Acc:0.78    LR:0.001\n",
      "No:260  Train_loss:7.084e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:261  Train_loss:7.090e-01  Cost:1.288e+00  Acc:0.78    LR:0.001\n",
      "No:262  Train_loss:7.095e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:263  Train_loss:7.085e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:264  Train_loss:7.075e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:265  Train_loss:7.089e-01  Cost:1.288e+00  Acc:0.78    LR:0.001\n",
      "No:266  Train_loss:7.082e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:267  Train_loss:7.076e-01  Cost:1.287e+00  Acc:0.78    LR:0.001\n",
      "No:268  Train_loss:7.081e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:269  Train_loss:7.087e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:270  Train_loss:7.092e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:271  Train_loss:7.075e-01  Cost:1.288e+00  Acc:0.78    LR:0.001\n",
      "No:272  Train_loss:7.078e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:273  Train_loss:7.068e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:274  Train_loss:7.078e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:275  Train_loss:7.069e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:276  Train_loss:7.088e-01  Cost:1.287e+00  Acc:0.78    LR:0.001\n",
      "No:277  Train_loss:7.059e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:278  Train_loss:7.077e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:279  Train_loss:7.067e-01  Cost:1.289e+00  Acc:0.78    LR:0.001\n",
      "No:280  Train_loss:7.056e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:281  Train_loss:7.065e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:282  Train_loss:7.045e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:283  Train_loss:7.057e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:284  Train_loss:7.048e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:285  Train_loss:7.056e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:286  Train_loss:7.056e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:287  Train_loss:7.038e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:288  Train_loss:7.045e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:289  Train_loss:7.048e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:290  Train_loss:7.051e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:291  Train_loss:7.033e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:292  Train_loss:7.040e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:293  Train_loss:7.050e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:294  Train_loss:7.042e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:295  Train_loss:7.042e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:296  Train_loss:7.036e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:297  Train_loss:7.045e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:298  Train_loss:7.038e-01  Cost:1.287e+00  Acc:0.78    LR:0.001\n",
      "No:299  Train_loss:7.027e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:300  Train_loss:7.048e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:301  Train_loss:7.037e-01  Cost:1.289e+00  Acc:0.78    LR:0.001\n",
      "No:302  Train_loss:7.031e-01  Cost:1.288e+00  Acc:0.78    LR:0.001\n",
      "No:303  Train_loss:7.035e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:304  Train_loss:7.043e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:305  Train_loss:7.032e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:306  Train_loss:7.037e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:307  Train_loss:7.038e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:308  Train_loss:7.027e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:309  Train_loss:7.021e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:310  Train_loss:7.039e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:311  Train_loss:7.020e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:312  Train_loss:7.034e-01  Cost:1.288e+00  Acc:0.77    LR:0.001\n",
      "No:313  Train_loss:7.022e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:314  Train_loss:7.022e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:315  Train_loss:7.025e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:316  Train_loss:7.021e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:317  Train_loss:7.029e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:318  Train_loss:7.023e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:319  Train_loss:7.002e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:320  Train_loss:7.019e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:321  Train_loss:7.010e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:322  Train_loss:7.021e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:323  Train_loss:7.008e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:324  Train_loss:7.007e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:325  Train_loss:7.005e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:326  Train_loss:7.015e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:327  Train_loss:7.024e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:328  Train_loss:6.996e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:329  Train_loss:7.011e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:330  Train_loss:7.006e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:331  Train_loss:7.023e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:332  Train_loss:7.012e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:333  Train_loss:7.013e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:334  Train_loss:7.004e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:335  Train_loss:7.005e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:336  Train_loss:6.989e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:337  Train_loss:7.001e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:338  Train_loss:7.005e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:339  Train_loss:6.990e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:340  Train_loss:7.000e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:341  Train_loss:6.983e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:342  Train_loss:6.984e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:343  Train_loss:6.989e-01  Cost:1.289e+00  Acc:0.77    LR:0.001\n",
      "No:344  Train_loss:6.981e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:345  Train_loss:6.993e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:346  Train_loss:6.991e-01  Cost:1.294e+00  Acc:0.77    LR:0.001\n",
      "No:347  Train_loss:6.999e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:348  Train_loss:7.004e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:349  Train_loss:6.977e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:350  Train_loss:6.976e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:351  Train_loss:6.981e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:352  Train_loss:6.985e-01  Cost:1.297e+00  Acc:0.77    LR:0.001\n",
      "No:353  Train_loss:6.979e-01  Cost:1.293e+00  Acc:0.78    LR:0.001\n",
      "No:354  Train_loss:6.981e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:355  Train_loss:6.983e-01  Cost:1.294e+00  Acc:0.77    LR:0.001\n",
      "No:356  Train_loss:6.962e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:357  Train_loss:6.988e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:358  Train_loss:6.977e-01  Cost:1.294e+00  Acc:0.77    LR:0.001\n",
      "No:359  Train_loss:6.984e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:360  Train_loss:6.986e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:361  Train_loss:6.976e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:362  Train_loss:6.968e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:363  Train_loss:6.976e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:364  Train_loss:6.956e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:365  Train_loss:6.973e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:366  Train_loss:6.972e-01  Cost:1.296e+00  Acc:0.77    LR:0.001\n",
      "No:367  Train_loss:6.966e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:368  Train_loss:6.961e-01  Cost:1.295e+00  Acc:0.77    LR:0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No:369  Train_loss:6.972e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:370  Train_loss:6.961e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:371  Train_loss:6.970e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:372  Train_loss:6.972e-01  Cost:1.295e+00  Acc:0.77    LR:0.001\n",
      "No:373  Train_loss:6.971e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:374  Train_loss:6.971e-01  Cost:1.290e+00  Acc:0.77    LR:0.001\n",
      "No:375  Train_loss:6.970e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:376  Train_loss:6.969e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:377  Train_loss:6.956e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:378  Train_loss:6.951e-01  Cost:1.295e+00  Acc:0.77    LR:0.001\n",
      "No:379  Train_loss:6.957e-01  Cost:1.294e+00  Acc:0.77    LR:0.001\n",
      "No:380  Train_loss:6.946e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:381  Train_loss:6.951e-01  Cost:1.294e+00  Acc:0.77    LR:0.001\n",
      "No:382  Train_loss:6.965e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:383  Train_loss:6.956e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:384  Train_loss:6.950e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:385  Train_loss:6.940e-01  Cost:1.294e+00  Acc:0.77    LR:0.001\n",
      "No:386  Train_loss:6.948e-01  Cost:1.291e+00  Acc:0.77    LR:0.001\n",
      "No:387  Train_loss:6.951e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:388  Train_loss:6.940e-01  Cost:1.295e+00  Acc:0.77    LR:0.001\n",
      "No:389  Train_loss:6.949e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:390  Train_loss:6.941e-01  Cost:1.294e+00  Acc:0.77    LR:0.001\n",
      "No:391  Train_loss:6.947e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:392  Train_loss:6.954e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:393  Train_loss:6.958e-01  Cost:1.295e+00  Acc:0.77    LR:0.001\n",
      "No:394  Train_loss:6.948e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:395  Train_loss:6.952e-01  Cost:1.294e+00  Acc:0.77    LR:0.001\n",
      "No:396  Train_loss:6.948e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:397  Train_loss:6.949e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n",
      "No:398  Train_loss:6.951e-01  Cost:1.292e+00  Acc:0.77    LR:0.001\n",
      "No:399  Train_loss:6.942e-01  Cost:1.293e+00  Acc:0.77    LR:0.001\n"
     ]
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1)  # use only 50% of memory\n",
    "#     self.session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "vali_cost =[]\n",
    "vali_acc = []\n",
    "train_cost = []\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as session:   # everything runs in this session\n",
    "#     with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())  # initialize all variables(trainable weights)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in range(global_step):                             # train for xx epochs\n",
    "        \n",
    "        if k<100:\n",
    "            learning_rate = init_learning_rate*(lr_decay**0)\n",
    "        elif k<200:\n",
    "            learning_rate = init_learning_rate*(lr_decay**1)\n",
    "        else:\n",
    "            learning_rate = init_learning_rate*(lr_decay**2)\n",
    "            \n",
    "        train_cost_i = 0\n",
    "        \n",
    "        for i in range(train_x.shape[0]//batch_size):  # iterate for trainNO/batchSize times(not necessarily visit every sample once)\n",
    "            batch_idx = np.random.choice(train_x.shape[0], batch_size, replace=False)  # select batch_size from train_num (no-replacement: no repetitive)\n",
    "            batch_xs = train_x[batch_idx]\n",
    "            batch_ys = train_y[batch_idx]\n",
    "            _, cost_val, acc_val = session.run([train_op, loss, accuracy], feed_dict = {x_holder: batch_xs,\n",
    "                                                            y_holder: batch_ys,\n",
    "                                                            lr_holder: learning_rate\n",
    "                                                            })\n",
    "            train_cost_i = train_cost_i + cost_val\n",
    "            \n",
    "        train_cost_i /= (i+1)\n",
    "            \n",
    "            \n",
    "        vali_cost_i, vali_acc_i = session.run([loss, accuracy], feed_dict = {x_holder: test_x, y_holder: test_y})\n",
    "        print('No:%3d  Train_loss:%.3e  Cost:%.3e  Acc:%.2f    LR:%.3f'  %(k, train_cost_i, vali_cost_i, vali_acc_i, learning_rate))\n",
    "        train_cost.append(train_cost_i)\n",
    "        vali_cost.append(vali_cost_i)\n",
    "        vali_acc.append(vali_acc_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_LearningCurve(vali_cost, vali_acc):\n",
    "    width = 12\n",
    "    height = 9\n",
    "    fig= plt.figure(figsize=(width, height))\n",
    "    ax = fig.add_subplot(1,1,1) #add_subplot(1,1,1)\n",
    "    \n",
    "    data_length=len(vali_cost)\n",
    "\n",
    "    plt.plot(np.arange(data_length), vali_cost, 'b-.', markersize=1,label= 'Validation Cost')\n",
    "    plt.plot(np.arange(data_length), (1-np.array([vali_acc])).reshape([data_length,]), 'r-', markersize=3, label= 'Validation Error-rate')\n",
    "\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    plt.xlabel('Num of Epochs')\n",
    "    plt.ylabel('Validation Accuracy and Cost')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsoAAAIaCAYAAAA5qGtvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3f865bc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_LearningCurve(vali_cost, vali_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
